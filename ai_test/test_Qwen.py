# test_Qwen.py
import argparse
import os
import time
import torch


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-dir", type=str, required=True, help="Local path to model folder")
    parser.add_argument("--prompt", type=str, default="Describe the final light of Owlsoft in poetic English.")
    parser.add_argument("--max-new-tokens", type=int, default=120)
    parser.add_argument("--temperature", type=float, default=0.8)
    parser.add_argument("--top-p", type=float, default=0.9)
    parser.add_argument("--repetition-penalty", type=float, default=1.1)
    args = parser.parse_args()

    print(f"ðŸ”§ CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ðŸ§  GPU: {torch.cuda.get_device_name(0)}  |  VRAM: {round(torch.cuda.get_device_properties(0).total_memory/1024**3, 2)} GB")

    # å»¶å¾ŒåŒ¯å…¥ï¼Œè®“éŒ¯èª¤è¨Šæ¯æ›´èšç„¦
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from auto_gptq import AutoGPTQForCausalLM

    model_dir = os.path.abspath(args.model_dir)
    if not os.path.isdir(model_dir):
        raise FileNotFoundError(f"Model dir not found: {model_dir}")

    # å˜—è©¦ä»¥ Transformers ç›´æŽ¥è¼‰å…¥ï¼ˆå¤šæ•¸ Qwen2.5 çš†å¯ï¼‰
    model = None
    tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

    try:
        # dtype ç”± transformers è¿‘æœŸç‰ˆæœ¬æŽ¡ç”¨ï¼›èˆŠç‰ˆæœƒå¿½ç•¥æ­¤åƒæ•¸
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            trust_remote_code=True,
            device_map="auto",   # è‡ªå‹•æ”¾åˆ° GPU/CPU
            dtype="auto"
        )
        backend = "transformers"
    except Exception as e:
        # è‹¥æ˜¯ GPTQ å°ˆæ¡ˆï¼Œå¯èƒ½éœ€è¦ AutoGPTQï¼›åšä¸€æ¬¡é™ç´šå˜—è©¦
        print(f"âš ï¸ Transformers load failed: {e}\nTrying AutoGPTQ fallbackâ€¦")
        try:
            model = AutoGPTQForCausalLM.from_quantized(
            model_dir,
            device="cuda:0" if torch.cuda.is_available() else "cpu",
            trust_remote_code=True,
            use_safetensors=True,
            quantize_config_file="quantization_config.json"  # ðŸ”¹ æ–°ç‰ˆåç¨±
        )

            backend = "auto-gptq"
        except Exception as e2:
            raise RuntimeError(f"Failed to load model with both Transformers and AutoGPTQ.\n"
                               f"Install auto-gptq if this is a GPTQ/Int4 model: pip install auto-gptq\nError: {e2}")

    # æº–å‚™ Promptï¼ˆé¿å… â€œAnswer:â€ è§¸ç™¼æ—©åœï¼Œä¸¦åŠ  English-only æŒ‡ç¤ºï¼‰
    system = "### Instruction: Answer in natural, fluent English only."
    user = args.prompt.strip()
    prompt = f"{system}\n### Input:\n{user}\n\n### Response:\n"

    inputs = tok(prompt, return_tensors="pt").to(model.device)

    gen_kwargs = dict(
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        do_sample=True,
        top_p=args.top_p,
        repetition_penalty=args.repetition_penalty,
        pad_token_id=tok.eos_token_id or tok.pad_token_id or 0,
        eos_token_id=tok.eos_token_id or tok.pad_token_id or 0,
        min_new_tokens=20,   # å¼·åˆ¶è‡³å°‘ç”Ÿæˆä¸€äº›å…§å®¹ï¼Œé¿å…ç©ºè¼¸å‡º
    )

    torch.set_grad_enabled(False)
    t0 = time.time()
    outputs = model.generate(**inputs, **gen_kwargs)
    dt = time.time() - t0

    text = tok.decode(outputs[0], skip_special_tokens=True)
    # åªå–ã€Œ### Response:ã€ä¹‹å¾Œçš„å…§å®¹
    if "### Response:" in text:
        text = text.split("### Response:", 1)[-1].strip()

    gen_tokens = outputs.shape[1] - inputs["input_ids"].shape[1]
    tok_per_s = gen_tokens / dt if dt > 0 else 0.0

    print("\n===== RESULT =====")
    print(text)
    print("\n===== STATS =====")
    print(f"Backend: {backend}")
    print(f"Generated tokens: {gen_tokens}")
    print(f"Time: {dt:.2f}s  |  Speed: {tok_per_s:.2f} tok/s")

if __name__ == "__main__":
    main()
